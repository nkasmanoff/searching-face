{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a search query, return all results which are related that query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from vectorize_dataset import load_descriptions_data, create_db\n",
    "\n",
    "import requests\n",
    "from helpers import clean_up_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRecommender:\n",
    "    def __init__(self, llm_backbone = OpenAI(), embeddings_backbone = OpenAIEmbeddings()):\n",
    "        self.llm_backbone = llm_backbone\n",
    "        self.embeddings_backbone = embeddings_backbone\n",
    "        self.hf_df = load_descriptions_data()\n",
    "        self.db = create_db(self.hf_df, self.embeddings_backbone)\n",
    "        self.datasets_url_base = \"https://huggingface.co/datasets/\"\n",
    "        # expose this index in a retriever interface\n",
    "        self.retriever = self.db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "        # create a chain to answer questions \n",
    "        self.qa = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm_backbone, chain_type=\"stuff\", retriever=self.retriever, return_source_documents=True)\n",
    "\n",
    "    def recommend_based_on_text(self, query):\n",
    "        result = self.qa({\"query\": query})\n",
    "        response_text = result['result']\n",
    "        source_documents = result['source_documents']\n",
    "        linked_datasets = [f\"{self.datasets_url_base}{x.metadata['id']}\" for x in source_documents]\n",
    "        return {'message': response_text, 'datasets': linked_datasets}\n",
    "\n",
    "    def get_similar_datasets(self, query_url):\n",
    "        retrieved_metadata = get_dataset_metadata(query_url)\n",
    "        cleaned_description = retrieved_metadata['description'] + clean_up_tags(retrieved_metadata['tags'])\n",
    "        similar_documents = database.similarity_search(cleaned_description)\n",
    "        similar_datasets = [f\"{self.datasets_url_base}{x.metadata['id']}\" for x in similar_documents if x.metadata['id'] not in url]       \n",
    "        return {'datasets': similar_datasets} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/noahkasmanoff/.cache/huggingface/datasets/nkasmanoff___parquet/nkasmanoff--huggingface-datasets-60bbbd3d2e18598e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 102.93it/s]\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1140, which is longer than the specified 1000\n",
      "Created a chunk of size 1140, which is longer than the specified 1000\n",
      "Created a chunk of size 1087, which is longer than the specified 1000\n",
      "Created a chunk of size 1190, which is longer than the specified 1000\n",
      "Created a chunk of size 1164, which is longer than the specified 1000\n",
      "Created a chunk of size 1190, which is longer than the specified 1000\n",
      "Created a chunk of size 1242, which is longer than the specified 1000\n",
      "Created a chunk of size 1187, which is longer than the specified 1000\n",
      "Created a chunk of size 1187, which is longer than the specified 1000\n",
      "Created a chunk of size 1480, which is longer than the specified 1000\n",
      "Created a chunk of size 1429, which is longer than the specified 1000\n",
      "Created a chunk of size 1184, which is longer than the specified 1000\n",
      "Created a chunk of size 1007, which is longer than the specified 1000\n",
      "Created a chunk of size 1184, which is longer than the specified 1000\n",
      "Created a chunk of size 1127, which is longer than the specified 1000\n",
      "Created a chunk of size 1112, which is longer than the specified 1000\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    }
   ],
   "source": [
    "db_lookup = DatasetRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': ' The HumAID Twitter dataset consists of several thousands of manually annotated tweets that has been collected during 19 major natural disaster events including earthquakes, hurricanes, wildfires, and floods, which happened from 2016 to 2019 across different parts of the World. The annotations in the provided datasets consists of humanitarian categories such as caution and advice, displaced people and evacuations, infrastructure and utility damage, injured or dead people, missing or found people, requests or urgent needs, rescue volunteering or donation effort, and sympathy and support. Additionally, the dataset contains the dataset contains 30,000 messages drawn from events including an earthquake in Haiti in 2010, an earthquake in Chile in 2010, floods in Pakistan in 2010, super-storm Sandy in the U.S.A. in 2012, and news articles spanning a large number of years and 100s of different disasters.',\n",
       " 'datasets': ['https://huggingface.co/datasets/disaster_response_messages',\n",
       "  'https://huggingface.co/datasets/Firoj/HumAID']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_lookup.recommend_based_on_text(\"Show me datasets which are about natural disasters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_api_url(url):\n",
    "    \"\"\"\n",
    "    This function checks to see if \"api\" is present in the URL between \".co\" and \"/datasets\". If not, it inserts \"api\" in the correct position.\n",
    "    \n",
    "    Args:\n",
    "    url (str): A URL string\n",
    "    \n",
    "    Returns:\n",
    "    str: A URL string with \"api\" inserted if necessary\n",
    "    \"\"\"\n",
    "    # Split the URL into three parts based on the location of \".co\" and \"/datasets\"\n",
    "    parts = url.split(\".co\")\n",
    "    first_part = parts[0] + \".co\"\n",
    "    last_part = parts[1]\n",
    "    last_parts = last_part.split(\"/datasets\")\n",
    "    middle_part = \"\"\n",
    "    if len(last_parts) > 1 and \"/api\" not in last_parts[0]:\n",
    "        middle_part = \"/api\"\n",
    "    # Concatenate the three parts to form the final URL\n",
    "    new_url = first_part + middle_part + last_parts[0] + \"/datasets\" + last_parts[1]\n",
    "    return new_url\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset_metadata(dataset_url):\n",
    "    retrieved_metadata = {}\n",
    "    dataset_url = check_api_url(dataset_url)\n",
    "    keys_to_retrieve = ['id','description', 'tags']\n",
    "    response = requests.get(dataset_url)\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        for key in keys_to_retrieve:\n",
    "            if key in response_json:\n",
    "                retrieved_metadata[key] = response_json[key]\n",
    "\n",
    "    return retrieved_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/turkic_xwmt\"\n",
    "retrieved_metadata = get_dataset_metadata(url)\n",
    "cleaned_description = retrieved_metadata['description'] + clean_up_tags(retrieved_metadata['tags'])\n",
    "similar_documents = database.similarity_search(cleaned_description)\n",
    "similar_datasets = [x.metadata['id'] for x in similar_documents if x.metadata['id'] not in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
